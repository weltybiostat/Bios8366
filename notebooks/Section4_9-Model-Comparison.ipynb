{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Model comparison\n",
    "\n",
    "To demonstrate the use of model comparison criteria, we implement the radon contamination example from the previous lecture. \n",
    "\n",
    "Below, we fit a **pooled model**, which assumes a single fixed effect across all counties, and a **hierarchical model** that allows for a random effect that partially pools the data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%matplotlib inline\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "import warnings\n",
    "warnings.filterwarnings(\"ignore\", module=\"mkl_fft\")\n",
    "sns.set_context('notebook')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The data include the observed radon levels and associated covariates for 85 counties in Minnesota."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "radon_data = pd.read_csv('../data/radon.csv', index_col=0)\n",
    "radon_data.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Pooled model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pymc3 import Model, sample, Normal, HalfCauchy, Uniform\n",
    "\n",
    "floor = radon_data.floor.values\n",
    "log_radon = radon_data.log_radon.values\n",
    "\n",
    "with Model() as pooled_model:\n",
    "    \n",
    "    β = Normal('β', 0, sd=1e5, shape=2)\n",
    "    σ = HalfCauchy('σ', 5)\n",
    "    \n",
    "    θ = β[0] + β[1]*floor\n",
    "    \n",
    "    y = Normal('y', θ, sd=σ, observed=log_radon)\n",
    "    \n",
    "    trace_p = sample(1000, tune=1000, njobs=2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pymc3 import traceplot\n",
    "\n",
    "traceplot(trace_p, varnames=['β']);"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Hierarchical model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "mn_counties = radon_data.county.unique()\n",
    "counties = mn_counties.shape[0]\n",
    "county = radon_data.county_code.values\n",
    "floor_measure = radon_data.floor.values\n",
    "                             \n",
    "with Model() as hierarchical_model:\n",
    "    \n",
    "    # Priors\n",
    "    μ_a = Normal('μ_a', mu=0., tau=0.0001)\n",
    "    σ_a = HalfCauchy('σ_a', 5)\n",
    "    \n",
    "    \n",
    "    # Random intercepts\n",
    "    a = Normal('a', mu=μ_a, sd=σ_a, shape=counties)\n",
    "    # Common slope\n",
    "    b = Normal('b', mu=0., sd=1e5)\n",
    "    \n",
    "    # Model error\n",
    "    sd_y = HalfCauchy('sd_y', 5)\n",
    "    \n",
    "    # Expected value\n",
    "    y_hat = a[county] + b * floor_measure\n",
    "    \n",
    "    # Data likelihood\n",
    "    y_like = Normal('y_like', mu=y_hat, sd=sd_y, observed=log_radon)\n",
    "    \n",
    "    trace_h = sample(1000, tune=1000, njobs=2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "traceplot(trace_h, varnames=['μ_a', 'b']);"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pymc3 import forestplot\n",
    "\n",
    "forestplot(trace_h, varnames=['a'], ylabels=['']);"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Widely-applicable Information Criterion (WAIC)\n",
    "\n",
    "WAIC (Watanabe 2010) is a fully Bayesian criterion for estimating out-of-sample expectation, using the computed log pointwise posterior predictive density (LPPD) and correcting for the effective number of parameters to adjust for overfitting."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pymc3 import waic\n",
    "\n",
    "pooled_waic = waic(trace_p, pooled_model)\n",
    "    \n",
    "pooled_waic.WAIC"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "hierarchical_waic = waic(trace_h, hierarchical_model)\n",
    "    \n",
    "hierarchical_waic.WAIC"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "PyMC3 includes two convenience functions to help compare WAIC for different models. The first of this functions is `compare`, this one computes WAIC (or LOO) from a set of traces and models and returns a DataFrame."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pymc3 import compare\n",
    "\n",
    "df_comp_WAIC = compare({hierarchical_model:trace_h, pooled_model:trace_p})\n",
    "df_comp_WAIC"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We have many columns so let check one by one the meaning of them:\n",
    "\n",
    "\n",
    "1. The first column contains the values of WAIC. The DataFrame is always sorted from lowest to highest WAIC. The index reflects the order in which the models are passed to this function.\n",
    "\n",
    "2. The second column is the estimated effective number of parameters. In general, models with more parameters will be more flexible to fit data and at the same time could also lead to overfitting. Thus we can interpret pWAIC as a penalization term, intuitively we can also interpret it as measure of how flexible each model is in fitting the data. \n",
    "\n",
    "3. The third column is the relative difference between the value of WAIC for the top-ranked model and the value of WAIC for each model. For this reason we will always get a value of 0 for the first model.\n",
    "\n",
    "4. Sometimes when comparing models, we do not want to select the \"best\" model, instead we want to perform predictions by averaging along all the models (or at least several models). Ideally we would like to perform a weighted average, giving more weight to the model that seems to explain/predict the data better. There are many approaches to perform this task, one of them is to use Akaike weights based on the values of WAIC for each model. These weights can be loosely interpreted as the probability of each model (among the compared models) given the data. One caveat of this approach is that the weights are based on point estimates of WAIC (i.e. the uncertainty is ignored).\n",
    "\n",
    "5. The fifth column records the standard error for the WAIC computations. The standard error can be useful to assess the uncertainty of the WAIC estimates. Nevertheless, caution need to be taken because the estimation of the standard error assumes normality and hence could be problematic when the sample size is low.\n",
    "\n",
    "6. In the same way that we can compute the standard error for each value of WAIC, we can compute the standard error of the differences between two values of WAIC. Notice that both quantities are not necessarily the same, the reason is that the uncertainty about WAIC is correlated between models. This quantity is always 0 for the top-ranked model.\n",
    "\n",
    "7. Finally we have the last column named \"warning\". A value of 1 indicates that the computation of WAIC may not be reliable, this warning is based on an empirical determined cutoff value and need to be interpreted with caution. For more details you can read this [paper](https://arxiv.org/abs/1507.04544)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The second convenience function takes the output of `compare` and produces a summary plot in the style of the one used in the book [Statistical Rethinking](http://xcelab.net/rm/statistical-rethinking/) by Richard McElreath (check also [this port](https://github.com/aloctavodia/Statistical-Rethinking-with-Python-and-PyMC3) of the examples in the book to PyMC3)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pymc3 import compareplot\n",
    "\n",
    "compareplot(df_comp_WAIC);"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- The empty circle represents the values of WAIC and the black error bars associated with them are the values of the standard deviation of WAIC. \n",
    "- The value of the lowest WAIC is also indicated with a vertical dashed grey line to ease comparison with other WAIC values.\n",
    "- The filled black dots are the in-sample deviance of each model, which for WAIC is  2 pWAIC from the corresponding WAIC value.\n",
    "\n",
    "For all models except the top-ranked one we also get a triangle indicating the value of the difference of WAIC between that model and the top model and a grey errobar indicating the standard error of the differences between the top-ranked WAIC and WAIC for each model."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Leave-one-out Cross-validation (LOO)\n",
    "\n",
    "LOO cross-validation is an estimate of the out-of-sample predictive fit. In cross-validation, the data are repeatedly partitioned into training and holdout sets, iteratively fitting the model with the former and evaluating the fit with the holdout data. Vehtari et al. (2016) introduced an efficient computation of LOO from MCMC samples, which are corrected using Pareto-smoothed importance sampling (PSIS) to provide an estimate of point-wise out-of-sample prediction accuracy."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pymc3 import loo\n",
    "\n",
    "pooled_loo = loo(trace_p, pooled_model)\n",
    "    \n",
    "pooled_loo.LOO"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "hierarchical_loo  = loo(trace_h, hierarchical_model)\n",
    "    \n",
    "hierarchical_loo.LOO"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can also use `compare` with LOO."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_comp_LOO = compare({hierarchical_model:trace_h, pooled_model:trace_p}, ic='LOO')\n",
    "df_comp_LOO"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The columns return the equivalent values for LOO, notice that in this example we get two warnings. Also notice that the order of the models is not the same as the one for WAIC.\n",
    "\n",
    "We can also plot the results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "compareplot(df_comp_LOO);"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Interpretation\n",
    "\n",
    "Though we might expect the hierarchical model to outperform a complete pooling model, there is little to choose between the models in this case, giving that both models gives very similar values of the information criteria. This is more clearly appreciated when we take into account the uncertainty (in terms of standard errors) of WAIC and LOO.\n",
    "\n",
    "---\n",
    "\n",
    "## Reference\n",
    "\n",
    "[Gelman, A., Hwang, J., & Vehtari, A. (2014). Understanding predictive information criteria for Bayesian models. Statistics and Computing, 24(6), 997–1016.](http://doi.org/10.1007/s11222-013-9416-2)\n",
    "\n",
    "[Vehtari, A, Gelman, A, Gabry, J. (2016). Practical Bayesian model evaluation using leave-one-out cross-validation and WAIC. Statistics and Computing](http://link.springer.com/article/10.1007/s11222-016-9696-4)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
